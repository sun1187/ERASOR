{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forget01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "metric_keys = [\n",
    "    'extraction_strength', 'extraction_strength_forget_para', 'extraction_strength_forget_para_pqa', 'extraction_strength_forget_para_pqpa', \n",
    "    'extraction_strength_ra', 'extraction_strength_retain', 'extraction_strength_retain_para', 'extraction_strength_retain_para_pqa',\n",
    "    'extraction_strength_retain_para_pqpa', 'extraction_strength_wf'\n",
    "]\n",
    "\n",
    "def get_all_checkpoints_eval_paths(task_dir):\n",
    "    eval_paths = []\n",
    "    if not os.path.exists(task_dir):\n",
    "        return eval_paths\n",
    "    for name in os.listdir(task_dir):\n",
    "        match = re.match(r\"checkpoint-(\\d+)\", name)\n",
    "        if match:\n",
    "            ckpt_num = int(match.group(1))\n",
    "            eval_file = os.path.join(task_dir, name, \"evals\", \"TOFU_SUMMARY.json\")\n",
    "            if os.path.exists(eval_file):\n",
    "                eval_paths.append((ckpt_num, eval_file))\n",
    "    return sorted(eval_paths, key=lambda x: x[0]) \n",
    "\n",
    "\n",
    "def epoch_alpha_beta(base_dir, models, trainers_experiments, epochs, alphas, betas, splits, metric_keys):\n",
    "    metrics_all = {key: [] for key in metric_keys}\n",
    "    combi_list = []\n",
    "\n",
    "    for model in models:\n",
    "        for trainer in trainers_experiments:\n",
    "            train_item_loss_type, train_item = trainer.split()\n",
    "            for epoch in epochs:\n",
    "                for alpha in alphas:\n",
    "                    for beta in betas:\n",
    "                        for split in splits:\n",
    "                            task_name = f\"tofu_{model}/{split}/{train_item_loss_type}/tofu_{model}_{split}_{train_item}_epoch{epoch}_alpha{alpha}_beta{beta}\"\n",
    "                            task_dir = os.path.join(base_dir, task_name)\n",
    "\n",
    "                            eval_paths = get_all_checkpoints_eval_paths(task_dir)\n",
    "\n",
    "                            if not eval_paths or len(eval_paths) != 4:\n",
    "                                print(f\"{task_dir} {len(eval_paths)}\")\n",
    "                                for key in metric_keys:\n",
    "                                    metrics_all[key].append(None)\n",
    "                                combi_list.append({\n",
    "                                    \"model\": model,\n",
    "                                    \"trainer\": trainer,\n",
    "                                    \"epoch\": epoch,\n",
    "                                    \"alpha\": alpha,\n",
    "                                    \"beta\": beta,\n",
    "                                    \"split\": split,\n",
    "                                    \"checkpoint\": None\n",
    "                                })\n",
    "                                continue\n",
    "\n",
    "                            for ckpt_num, json_path in eval_paths:\n",
    "                                try:\n",
    "                                    with open(json_path, 'r') as f:\n",
    "                                        data = json.load(f)\n",
    "                                        for key in metric_keys:\n",
    "                                            value = data.get(key, None)\n",
    "                                            if value is None:\n",
    "                                                print(f\"{key}: {json_path}\")\n",
    "                                            metrics_all[key].append(value)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"{json_path}: {e}\")\n",
    "                                    for key in metric_keys:\n",
    "                                        metrics_all[key].append(None)\n",
    "\n",
    "                                combi_list.append({\n",
    "                                    \"model\": model,\n",
    "                                    \"trainer\": trainer,\n",
    "                                    \"epoch\": epoch,\n",
    "                                    \"alpha\": alpha,\n",
    "                                    \"beta\": beta,\n",
    "                                    \"split\": split,\n",
    "                                    \"checkpoint\": ckpt_num\n",
    "                                })\n",
    "\n",
    "    return metrics_all, combi_list\n",
    "\n",
    "\n",
    "def epoch_alpha(base_dir, models, trainers_experiments, epochs, alphas, splits, metric_keys):\n",
    "    metrics_all = {key: [] for key in metric_keys}\n",
    "    combi_list = []\n",
    "\n",
    "    for model in models:\n",
    "        for trainer in trainers_experiments:\n",
    "            train_item_loss_type, train_item = trainer.split()\n",
    "            for epoch in epochs:\n",
    "                for alpha in alphas:\n",
    "                    for split in splits:\n",
    "                        task_name = f\"tofu_{model}/{split}/{train_item_loss_type}/tofu_{model}_{split}_{train_item}_epoch{epoch}_alpha{alpha}\"\n",
    "                        task_dir = os.path.join(base_dir, task_name)\n",
    "\n",
    "                        eval_paths = get_all_checkpoints_eval_paths(task_dir)\n",
    "                    \n",
    "                        if not eval_paths or len(eval_paths) != 4:\n",
    "                            print(f\"{task_dir} {len(eval_paths)}\")\n",
    "                            for key in metric_keys:\n",
    "                                metrics_all[key].append(None)\n",
    "                            combi_list.append({\n",
    "                                \"model\": model,\n",
    "                                \"trainer\": trainer,\n",
    "                                \"epoch\": epoch,\n",
    "                                \"alpha\": alpha,\n",
    "                                \"split\": split,\n",
    "                                \"checkpoint\": None\n",
    "                            })\n",
    "                            continue\n",
    "\n",
    "                        for ckpt_num, json_path in eval_paths:\n",
    "                            try:\n",
    "                                with open(json_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                    for key in metric_keys:\n",
    "                                        value = data.get(key, None)\n",
    "                                        if value is None:\n",
    "                                            print(f\"{key}: {json_path}\")\n",
    "                                        metrics_all[key].append(value)\n",
    "                            except Exception as e:\n",
    "                                print(f\" {json_path}: {e}\")\n",
    "                                for key in metric_keys:\n",
    "                                    metrics_all[key].append(None)\n",
    "\n",
    "                            combi_list.append({\n",
    "                                \"model\": model,\n",
    "                                \"trainer\": trainer,\n",
    "                                \"epoch\": epoch,\n",
    "                                \"alpha\": alpha,\n",
    "                                \"split\": split,\n",
    "                                \"checkpoint\": ckpt_num\n",
    "                            })\n",
    "\n",
    "    return metrics_all, combi_list\n",
    "\n",
    "\n",
    "def epoch(base_dir, models, trainers_experiments, epochs, splits, metric_keys):\n",
    "    metrics_all = {key: [] for key in metric_keys}\n",
    "    combi_list = []\n",
    "\n",
    "    for model in models:\n",
    "        for trainer in trainers_experiments:\n",
    "            train_item_loss_type, train_item = trainer.split()\n",
    "            for epoch in epochs:\n",
    "                for split in splits:\n",
    "                    task_name = f\"tofu_{model}/{split}/{train_item_loss_type}/tofu_{model}_{split}_{train_item}_epoch{epoch}\"\n",
    "                    task_dir = os.path.join(base_dir, task_name)\n",
    "\n",
    "                    eval_paths = get_all_checkpoints_eval_paths(task_dir)\n",
    "                    \n",
    "                    if not eval_paths or len(eval_paths) != 4:\n",
    "                        print(f\"{task_dir} {len(eval_paths)}\")\n",
    "                        for key in metric_keys:\n",
    "                            metrics_all[key].append(None)\n",
    "                        combi_list.append({\n",
    "                            \"model\": model,\n",
    "                            \"trainer\": trainer,\n",
    "                            \"epoch\": epoch,\n",
    "                            \"split\": split,\n",
    "                            \"checkpoint\": None\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    for ckpt_num, json_path in eval_paths:\n",
    "                        try:\n",
    "                            with open(json_path, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                                for key in metric_keys:\n",
    "                                    value = data.get(key, None)\n",
    "                                    if value is None:\n",
    "                                        print(f\"{key}: {json_path}\")\n",
    "                                    metrics_all[key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"{json_path}: {e}\")\n",
    "                            for key in metric_keys:\n",
    "                                metrics_all[key].append(None)\n",
    "\n",
    "                        combi_list.append({\n",
    "                            \"model\": model,\n",
    "                            \"trainer\": trainer,\n",
    "                            \"epoch\": epoch,\n",
    "                            \"split\": split,\n",
    "                            \"checkpoint\": ckpt_num\n",
    "                        })\n",
    "\n",
    "    return metrics_all, combi_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'yourpath/saves/unlearn'\n",
    "models = [\"Llama-3.1-8B-Instruct\"]\n",
    "splits = [\"forget01\"]  # Can change to forget05, forget10\n",
    "\n",
    "trainers_experiments = [\"DPO_GDR DPO\"]\n",
    "epochs = [\"4\"]\n",
    "alphas = [\"1\", \"2\", \"10\", \"20\"]\n",
    "betas = [\"0.1\", \"1\", \"10\"]\n",
    "dpo_gdr_result, dpo_gdpr_combo = epoch_alpha_beta(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, alphas=alphas, betas=betas, splits=splits, metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "trainers_experiments = [\"DPO_KL DPO\"]\n",
    "epochs = [\"4\"]\n",
    "alphas = [\"1\", \"2\", \"10\", \"20\"]\n",
    "betas = [\"0.1\", \"1\", \"10\"]\n",
    "dpo_kl_result, dpo_kl_combo = epoch_alpha_beta(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, alphas=alphas, betas=betas, splits=splits, metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "trainers_experiments = [\"NPO_GDR NPO\"]\n",
    "epochs = [\"4\"]\n",
    "alphas = [\"1\", \"2\", \"10\", \"20\"]\n",
    "betas = [\"0.1\", \"1\", \"10\"]\n",
    "npo_gdr_result, npo_gdpr_combo = epoch_alpha_beta(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, alphas=alphas, betas=betas, splits=splits, metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "trainers_experiments = [\"NPO_KL NPO\"]\n",
    "epochs = [\"4\"]\n",
    "alphas = [\"1\", \"2\", \"10\", \"20\"]\n",
    "betas = [\"0.1\", \"1\", \"10\"]\n",
    "npo_kl_result, npo_kl_combo = epoch_alpha_beta(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, alphas=alphas, betas=betas, splits=splits, metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "trainers_experiments = [\"GradAscent GradAscent\"]\n",
    "epochs = [\"4\"]\n",
    "ga_result, ga_combo = epoch(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, splits=splits, metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "trainers_experiments = [\"GradDiff_GDR GradDiff\"]\n",
    "epochs = [\"4\"]\n",
    "alphas = [\"1\", \"2\", \"10\", \"20\"]\n",
    "gd_gdr_result, gd_gdr_combo = epoch_alpha(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, alphas=alphas, splits=splits, metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "trainers_experiments = [\"GradDiff_KL GradDiff\"]\n",
    "epochs = [\"4\"]\n",
    "alphas = [\"1\", \"2\", \"10\", \"20\"]\n",
    "gd_kl_result, gd_kl_combo = epoch_alpha(\n",
    "    base_dir=base_dir, models=models, trainers_experiments=trainers_experiments,\n",
    "    epochs=epochs, alphas=alphas, splits=splits, metric_keys=metric_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_path = \"yourpath/saves/finetune/tofu_Llama-3.1-8B-Instruct_retain99/evals/TOFU_SUMMARY.json\"  # Can change to retain95, retain90\n",
    "full_path = \"yourpath/saves/finetune/tofu_Llama-3.1-8B-Instruct_full/evals_forget01/TOFU_SUMMARY.json\"  # Can change to forget05, forget10\n",
    "\n",
    "with open(retain_path, \"r\") as f:\n",
    "    retain_metrics = json.load(f)\n",
    "with open(full_path, \"r\") as f:\n",
    "    full_metrics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_utility = full_metrics['extraction_strength_retain']\n",
    "threshold = full_model_utility * 0.95\n",
    "print(f\"[INFO] Full of extraction_strength_retain: {full_model_utility:.6f}\")\n",
    "print(f\"[INFO] Threshold for extraction_strength_retain (95% of full): {threshold:.6f}\")\n",
    "print()\n",
    "\n",
    "def find_best_result(metrics_dict, combi_list, threshold, name):\n",
    "    best_alpha = None\n",
    "    best_fq = float('inf')\n",
    "    best_mu = None\n",
    "\n",
    "    for alpha, fq, mu in zip(combi_list, metrics_dict[\"extraction_strength\"], metrics_dict[\"extraction_strength_retain\"]):\n",
    "        if mu is not None and fq is not None and mu >= threshold:\n",
    "            if fq <= best_fq:\n",
    "                best_fq = fq\n",
    "                best_alpha = alpha\n",
    "                best_mu = mu\n",
    "\n",
    "    if best_alpha is None:\n",
    "        print(f\"[WARN] {name} No alpha satisfying the threshold. Using fallback: max extraction_strength_retain.\")\n",
    "        max_idx = None\n",
    "        max_mu = -float('inf')\n",
    "\n",
    "        for idx, mu in enumerate(metrics_dict[\"extraction_strength_retain\"]):\n",
    "            if mu is not None and mu > max_mu:\n",
    "                max_mu = mu\n",
    "                max_idx = idx\n",
    "\n",
    "        if max_idx is not None:\n",
    "            best_alpha = combi_list[max_idx]\n",
    "            best_mu = metrics_dict[\"extraction_strength_retain\"][max_idx]\n",
    "            best_fq = metrics_dict[\"extraction_strength\"][max_idx]\n",
    "        else:\n",
    "            best_alpha = {}\n",
    "            best_mu = None\n",
    "            best_fq = None\n",
    "\n",
    "    return best_alpha, best_fq, best_mu\n",
    "\n",
    "\n",
    "alpha_dpo_gdr, fq_dpo_gdr, mu_dpo_gdr = find_best_result(dpo_gdr_result, dpo_gdpr_combo, threshold, 'dpo_gdpr')\n",
    "alpha_dpo_kl, fq_dpo_kl, mu_dpo_kl = find_best_result(dpo_kl_result, dpo_kl_combo, threshold, 'dpo_kl')\n",
    "alpha_npo_gdr, fq_npo_gdr, mu_npo_gdr = find_best_result(npo_gdr_result, npo_gdpr_combo, threshold, 'npo_gdr')\n",
    "alpha_npo_kl, fq_npo_kl, mu_npo_kl = find_best_result(npo_kl_result, npo_kl_combo, threshold, 'npo_kl')\n",
    "alpha_ga, fq_ga, mu_ga = find_best_result(ga_result, ga_combo, threshold, 'ga')\n",
    "alpha_gd_gdr, fq_gd_gdr, mu_gd_gdr = find_best_result(gd_gdr_result, gd_gdr_combo, threshold, 'gd_gdr')\n",
    "alpha_gd_kl, fq_gd_kl, mu_gd_kl = find_best_result(gd_kl_result, gd_kl_combo, threshold, 'gd_kl')\n",
    "\n",
    "\n",
    "print(f\"- DPO-GDR     : forget = {fq_dpo_gdr}, retain = {mu_dpo_gdr},  alpha = {alpha_dpo_gdr.values()}\")\n",
    "print(f\"- DPO-KL      : forget = {fq_dpo_kl}, retain = {mu_dpo_kl},  alpha = {alpha_dpo_kl.values()}\")\n",
    "print(f\"- NPO-GDR     : forget = {fq_npo_gdr}, retain = {mu_npo_gdr},  alpha = {alpha_npo_gdr.values()}\")\n",
    "print(f\"- NPO-KL      : forget = {fq_npo_kl}, retain = {mu_npo_kl},  alpha = {alpha_npo_kl.values()}\")\n",
    "print(f\"- GA          : forget = {fq_ga}, retain = {mu_ga},  alpha = {alpha_ga.values()}\")\n",
    "print(f\"- GD-GDR      : forget = {fq_gd_gdr}, retain = {mu_gd_gdr},  alpha = {alpha_gd_gdr.values()}\")\n",
    "print(f\"- GD-KL       : forget = {fq_gd_kl}, retain = {mu_gd_kl},  alpha = {alpha_gd_kl.values()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_utility = full_metrics['extraction_strength_retain']\n",
    "threshold = full_model_utility * 0.90\n",
    "print(f\"[INFO] Full of extraction_strength_retain: {full_model_utility:.6f}\")\n",
    "print(f\"[INFO] Threshold for extraction_strength_retain (90% of full): {threshold:.6f}\")\n",
    "print()\n",
    "\n",
    "def find_best_result(metrics_dict, combi_list, threshold, name):\n",
    "    best_alpha = None\n",
    "    best_fq = float('inf')\n",
    "    best_mu = None\n",
    "\n",
    "    for alpha, fq, mu in zip(combi_list, metrics_dict[\"extraction_strength\"], metrics_dict[\"extraction_strength_retain\"]):\n",
    "        if mu is not None and fq is not None and mu >= threshold:\n",
    "            if fq <= best_fq:\n",
    "                best_fq = fq\n",
    "                best_alpha = alpha\n",
    "                best_mu = mu\n",
    "\n",
    "    if best_alpha is None:\n",
    "        print(f\"[WARN] {name} No alpha satisfying the threshold. Using fallback: max extraction_strength_retain.\")\n",
    "        max_idx = None\n",
    "        max_mu = -float('inf')\n",
    "\n",
    "        for idx, mu in enumerate(metrics_dict[\"extraction_strength_retain\"]):\n",
    "            if mu is not None and mu > max_mu:\n",
    "                max_mu = mu\n",
    "                max_idx = idx\n",
    "\n",
    "        if max_idx is not None:\n",
    "            best_alpha = combi_list[max_idx]\n",
    "            best_mu = metrics_dict[\"extraction_strength_retain\"][max_idx]\n",
    "            best_fq = metrics_dict[\"extraction_strength\"][max_idx]\n",
    "        else:\n",
    "            best_alpha = {}\n",
    "            best_mu = None\n",
    "            best_fq = None\n",
    "\n",
    "    return best_alpha, best_fq, best_mu\n",
    "\n",
    "alpha_dpo_gdr, fq_dpo_gdr, mu_dpo_gdr = find_best_result(dpo_gdr_result, dpo_gdpr_combo, threshold, 'dpo_gdpr')\n",
    "alpha_dpo_kl, fq_dpo_kl, mu_dpo_kl = find_best_result(dpo_kl_result, dpo_kl_combo, threshold, 'dpo_kl')\n",
    "alpha_npo_gdr, fq_npo_gdr, mu_npo_gdr = find_best_result(npo_gdr_result, npo_gdpr_combo, threshold, 'npo_gdr')\n",
    "alpha_npo_kl, fq_npo_kl, mu_npo_kl = find_best_result(npo_kl_result, npo_kl_combo, threshold, 'npo_kl')\n",
    "alpha_ga, fq_ga, mu_ga = find_best_result(ga_result, ga_combo, threshold, 'ga')\n",
    "alpha_gd_gdr, fq_gd_gdr, mu_gd_gdr = find_best_result(gd_gdr_result, gd_gdr_combo, threshold, 'gd_gdr')\n",
    "alpha_gd_kl, fq_gd_kl, mu_gd_kl = find_best_result(gd_kl_result, gd_kl_combo, threshold, 'gd_kl')\n",
    "\n",
    "print(f\"- DPO-GDR     : forget = {fq_dpo_gdr}, retain = {mu_dpo_gdr},  alpha = {alpha_dpo_gdr.values()}\")\n",
    "print(f\"- DPO-KL      : forget = {fq_dpo_kl}, retain = {mu_dpo_kl},  alpha = {alpha_dpo_kl.values()}\")\n",
    "print(f\"- NPO-GDR     : forget = {fq_npo_gdr}, retain = {mu_npo_gdr},  alpha = {alpha_npo_gdr.values()}\")\n",
    "print(f\"- NPO-KL      : forget = {fq_npo_kl}, retain = {mu_npo_kl},  alpha = {alpha_npo_kl.values()}\")\n",
    "print(f\"- GA          : forget = {fq_ga}, retain = {mu_ga},  alpha = {alpha_ga.values()}\")\n",
    "print(f\"- GD-GDR      : forget = {fq_gd_gdr}, retain = {mu_gd_gdr},  alpha = {alpha_gd_gdr.values()}\")\n",
    "print(f\"- GD-KL       : forget = {fq_gd_kl}, retain = {mu_gd_kl},  alpha = {alpha_gd_kl.values()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "target_alphas = {\n",
    "    \"GA\": alpha_ga,\n",
    "    \"GD_GDR\": alpha_gd_gdr,\n",
    "    \"GD_KL\": alpha_gd_kl,    \n",
    "    \"DPO_GDR\": alpha_dpo_gdr,\n",
    "    \"DPO_KL\": alpha_dpo_kl,\n",
    "    \"NPO_GDR\": alpha_npo_gdr,\n",
    "    \"NPO_KL\": alpha_npo_kl,\n",
    "}\n",
    "\n",
    "metrics_combo_dicts = {\n",
    "    \"GA\": (ga_result, ga_combo),\n",
    "    \"GD_GDR\": (gd_gdr_result, gd_gdr_combo),\n",
    "    \"GD_KL\": (gd_kl_result, gd_kl_combo),\n",
    "    \"DPO_GDR\": (dpo_gdr_result, dpo_gdpr_combo),\n",
    "    \"DPO_KL\": (dpo_kl_result, dpo_kl_combo),\n",
    "    \"NPO_GDR\": (npo_gdr_result, npo_gdpr_combo),\n",
    "    \"NPO_KL\": (npo_kl_result, npo_kl_combo),\n",
    "}\n",
    "\n",
    "\n",
    "metric_keys = [\n",
    "    'extraction_strength_retain', 'extraction_strength_retain_para_pqa',\n",
    "    \"extraction_strength_ra\", \"extraction_strength_wf\",\n",
    "    'extraction_strength', 'extraction_strength_forget_para_pqa'\n",
    "]\n",
    "\n",
    "\n",
    "def get_all_metrics(metrics_dict, combi_list, target_alpha, metric_keys):\n",
    "    results = {}\n",
    "    if target_alpha not in combi_list:\n",
    "        return {k: None for k in metric_keys}\n",
    "\n",
    "    idx = combi_list.index(target_alpha)\n",
    "\n",
    "    for key in metric_keys:\n",
    "        vals = metrics_dict.get(key, [None]*len(combi_list))\n",
    "        results[key] = vals[idx] if idx < len(vals) else None\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_metrics_direct(metrics_dict, metric_keys):\n",
    "    row = {}\n",
    "    for key in metric_keys:\n",
    "        row[key] = metrics_dict.get(key, None)\n",
    "    return row\n",
    "\n",
    "rows = []\n",
    "\n",
    "\n",
    "rows.append({\n",
    "    \"Method\": \"RETAIN\",\n",
    "    \"Alpha\": \"N/A\",\n",
    "    **extract_metrics_direct(retain_metrics, metric_keys)\n",
    "})\n",
    "\n",
    "rows.append({\n",
    "    \"Method\": \"FULL\",\n",
    "    \"Alpha\": \"N/A\",\n",
    "    **extract_metrics_direct(full_metrics, metric_keys)\n",
    "})\n",
    "\n",
    "\n",
    "for method, alpha in target_alphas.items():\n",
    "    metrics_dict, combi_list = metrics_combo_dicts[method]\n",
    "    row = {\"Method\": method, \"Alpha\": alpha}\n",
    "    row.update(get_all_metrics(metrics_dict, combi_list, alpha, metric_keys))\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tofu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
